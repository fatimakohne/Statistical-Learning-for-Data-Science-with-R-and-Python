plot(prune.credit)
text(prune.credit, pretty = 0)
# use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Bagging and Random Forests
#Bagging
bag.credit = randomForest(Balance ~., data=data.1, subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
# Regression Tree
# Loading data
library(ISLR)
library(tree)
library(MASS)
library(randomForest)
library(tidyverse)
set.seed(6822651)
#Loading data
credit_data= read.csv("Credit.csv")
# checking the missing value
sum(is.na(credit_data)) # ' there is no missing value '
## Transform data
data.1 <- credit_data[, 3:13]
### Transform variables
data.1 <- data.1 %>%
mutate(
Gender = factor(Gender),
Student = factor(Student),
Married = factor(Married),
Ethnicity = factor(Ethnicity),
)
#Create training and test set
train =sample(1:nrow(data.1), nrow(data.1)*0.5)
data.train = data.1[train, ]
data.test = data.1[-train, ]
#Fit a regression tree to the training set.
tree.credit =tree(Balance~., data = data.train)
summary(tree.credit)
#Plot the tree
plot(tree.credit)
text(tree.credit, pretty = 0)
#plot of the prediction and Calculating test-MSE
y.test = data.test$Balance
pred.credit = predict(tree.credit, data.test)
MSE.test = mean((y.test - pred.credit)^2)
MSE.test
((MSE.test)^0.5)
# Cross-validation  in order to determine the optimal level of tree complexity
cv.credit = cv.tree(tree.credit, FUN = prune.tree)
par(mfrow = c(1,2))
plot(cv.credit$size, cv.credit$dev, type = "b")
plot(cv.credit$k, cv.credit$dev, type = "b")
# Re-fitting the tree with 5 nodes
prune.credit = prune.tree(tree.credit, best =5 )
plot(prune.credit)
text(prune.credit, pretty = 0)
# use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Pruning the tree causes the test-MSE to increase to 46776.09
########
#Bagging and Random Forests
#Bagging
bag.credit = randomForest(Balance ~., data=data.1, subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
# Regression Tree
# Loading data
library(ISLR)
library(tree)
library(MASS)
library(randomForest)
library(tidyverse)
set.seed(6822651)
#Loading data
credit_data= read.csv("Credit.csv")
# checking the missing value
sum(is.na(credit_data)) # ' there is no missing value '
## Transform data
data.1 <- credit_data[, 3:13]
### Transform variables
data.1 <- data.1 %>%
mutate(
Gender = factor(Gender),
Student = factor(Student),
Married = factor(Married),
Ethnicity = factor(Ethnicity),
)
#Create training and test set
train =sample(1:nrow(data.1), nrow(data.1)*0.5)
data.train = data.1[train, ]
data.test = data.1[-train, ]
#Fit a regression tree to the training set.
tree.credit =tree(Balance~., data = data.train)
summary(tree.credit)
#Plot the tree
plot(tree.credit)
text(tree.credit, pretty = 0)
#plot of the prediction and Calculating test-MSE
y.test = data.test$Balance
pred.credit = predict(tree.credit, data.test)
MSE.test = mean((y.test - pred.credit)^2)
MSE.test
((MSE.test)^0.5)
# Cross-validation  in order to determine the optimal level of tree complexity
cv.credit = cv.tree(tree.credit, FUN = prune.tree)
par(mfrow = c(1,2))
plot(cv.credit$size, cv.credit$dev, type = "b")
plot(cv.credit$k, cv.credit$dev, type = "b")
# Re-fitting the tree with 5 nodes
prune.credit = prune.tree(tree.credit, best =5 )
plot(prune.credit)
text(prune.credit, pretty = 0)
# use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Pruning the tree causes the test-MSE to increase to 46776.09
########
#Bagging and Random Forests
#Bagging
bag.credit = randomForest(Balance ~., data=data.1, subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
library(ISLR)
library(e1071)
library(tidyverse)
set.seed(6822651)
#Loading data
data1= read.csv("Credit.csv")
#Create a binary variable
Y = ifelse(data1$Balance > median(data1$Balance), 1, 0)
data1$Balancebin= as.factor(Y)
svm.cv = tune(svm, Balancebin ~ ., data = data1, kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(svm.cv)
#the models Fitting
svm.lin = svm(Balancebin ~., data = data1, kernel = "linear", cost = 0.1)
summary(svm.lin)
#Creating confusions a matricend calculating error rates
Y.hat.lin = predict(svm.lin, data1)
conf.mat.lin = table(data1$Balancebin, Y.hat.lin)
#Confusion matrice
conf.mat.lin
### Calculating test error rates
err.lin = 1 - sum(diag(conf.mat.lin)) / sum(conf.mat.lin)
err.lin
# Support Vector Clasifier
library(ISLR)
library(e1071)
library(tidyverse)
set.seed(6822651)
#Loading data
data1= read.csv("Credit.csv")
#Create a binary variable
Y = ifelse(data1$Balance > median(data1$Balance), 1, 0)
data1$Balancebin= as.factor(Y)
#Fit a support vector classifier to the data with various values of cost,
#in order to predict whether a customer gets high or low average credit card balance
svm.cv = tune(svm, Balancebin ~ ., data = data1, kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(svm.cv)
#the models Fitting
svm.lin = svm(Balancebin ~., data = data1, kernel = "linear", cost = 0.1)
summary(svm.lin)
#Creating confusions a matricend calculating error rates
Y.hat.lin = predict(svm.lin, data1)
conf.mat.lin = table(data1$Balancebin, Y.hat.lin)
#Confusion matrice
conf.mat.lin
### Calculating test error rates
err.lin = 1 - sum(diag(conf.mat.lin)) / sum(conf.mat.lin)
err.lin
# Loading data
library(ISLR)
library(tree)
library(MASS)
library(randomForest)
library(tidyverse)
set.seed(6822651)
#Loading data
credit_data= read.csv("Credit.csv")
# checking the missing value
sum(is.na(credit_data)) # ' there is no missing value '
## Transform data
data1 <- credit_data[, 3:13]
### Transform variables
data1 <- data1 %>%
mutate(
Gender = factor(Gender),
Student = factor(Student),
Married = factor(Married),
Ethnicity = factor(Ethnicity),
)
#Create training and test set
train =sample(1:nrow(data.1), nrow(data1)*0.5)
#Create training and test set
train =sample(1:nrow(data1), nrow(data1)*0.5)
data.train = data1[train, ]
data.test = data1[-train, ]
tree.credit =tree(Balance~., data = data.train)
summary(tree.credit)
#Plot the tree
plot(tree.credit)
text(tree.credit, pretty = 0)
#plot of the prediction and Calculating test-MSE
y.test = data.test$Balance
pred.credit = predict(tree.credit, data.test)
MSE.test = mean((y.test - pred.credit)^2)
MSE.test
((MSE.test)^0.5)
# Cross-validation  in order to determine the optimal level of tree complexity
cv.credit = cv.tree(tree.credit, FUN = prune.tree)
par(mfrow = c(1,2))
plot(cv.credit$size, cv.credit$dev, type = "b")
plot(cv.credit$k, cv.credit$dev, type = "b")
# Re-fitting the tree with 5 nodes
prune.credit = prune.tree(tree.credit, best =5 )
plot(prune.credit)
text(prune.credit, pretty = 0)
# use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Bagging
bag.credit = randomForest(Balance ~., data=data1, subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
library(ISLR)
library(e1071)
set.seed(6822651)
#Loading data
data2= read.csv("Credit.csv")
#Create a binary variable
Y = ifelse(data2$Balance > median(data2$Balance), 1, 0)
data2$Balancebin= as.factor(Y)
svm.cv = tune(svm, Balancebin ~ ., data = data2, kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(svm.cv)
#the models Fitting
svm.lin = svm(Balancebin ~., data = data2, kernel = "linear", cost = 0.1)
summary(svm.lin)
#Creating confusions a matricend calculating error rates
Y.hat.lin = predict(svm.lin, data2)
conf.mat.lin = table(data2$Balancebin, Y.hat.lin)
#Confusion matrice
conf.mat.lin
### Calculating test error rates
err.lin = 1 - sum(diag(conf.mat.lin)) / sum(conf.mat.lin)
err.lin
# Loading data
library(ISLR)
library(tree)
library(MASS)
library(randomForest)
library(tidyverse)
set.seed(6822651)
#Loading data
credit_data= read.csv("Credit.csv")
# checking the missing value
sum(is.na(credit_data)) # ' there is no missing value '
## Transform data
data1 <- credit_data[, 3:13]
### Transform variables
data1 <- data1 %>%
mutate(
Gender = factor(Gender),
Student = factor(Student),
Married = factor(Married),
Ethnicity = factor(Ethnicity),
)
#Create training and test set
train =sample(1:nrow(data1), nrow(data1)*0.5)
data.train = data1[train, ]
data.test = data1[-train, ]
#Fit a regression tree to the training set.
tree.credit =tree(Balance~., data = data.train)
summary(tree.credit)
#Plot the tree
plot(tree.credit)
text(tree.credit, pretty = 0)
#Calculating test-MSE
y.test = data.test$Balance
pred.credit = predict(tree.credit, data.test)
MSE.test = mean((y.test - pred.credit)^2)
MSE.test
((MSE.test)^0.5)
# Cross-validation
cv.credit = cv.tree(tree.credit, FUN = prune.tree)
par(mfrow = c(1,2))
plot(cv.credit$size, cv.credit$dev, type = "b")
plot(cv.credit$k, cv.credit$dev, type = "b")
# Re-fitting the tree with 5 nodes
prune.credit = prune.tree(tree.credit, best =5 )
plot(prune.credit)
text(prune.credit, pretty = 0)
# Use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Fit the model with Bagging
bag.credit = randomForest(Balance ~., data=data1,
subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
library(ISLR)
library(e1071)
set.seed(6822651)
#Loading data
data2= read.csv("Credit.csv")
#Create a binary variable
Y = ifelse(data2$Balance > median(data2$Balance), 1, 0)
data2$Balancebin= as.factor(Y)
#Fit a support vector classifier
svm.cv = tune(svm, Balancebin ~ ., data = data2, kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(svm.cv)
# Fit the model
svm.lin = svm(Balancebin ~., data = data2, kernel = "linear", cost = 0.1)
summary(svm.lin)
#Creating confusions a matricend calculating error rates
Y.hat.lin = predict(svm.lin, data2)
conf.mat.lin = table(data2$Balancebin, Y.hat.lin)
#Confusion matrice
conf.mat.lin
### Calculating test error rates
err.lin = 1 - sum(diag(conf.mat.lin)) / sum(conf.mat.lin)
err.lin
# REGRESSION TREE
# Loading data
library(ISLR)
library(tree)
library(MASS)
library(randomForest)
library(tidyverse)
set.seed(6822651)
#Loading data
credit_data= read.csv("Credit.csv")
# checking the missing value
sum(is.na(credit_data)) # ' there is no missing value '
## Transform data
data1 <- credit_data[, 3:13]
### Transform variables
data1 <- data1 %>%
mutate(
Gender = factor(Gender),
Student = factor(Student),
Married = factor(Married),
Ethnicity = factor(Ethnicity),
)
#Create training and test set
train =sample(1:nrow(data1), nrow(data1)*0.5)
data.train = data1[train, ]
data.test = data1[-train, ]
#Fit a regression tree to the training set.
tree.credit =tree(Balance~., data = data.train)
summary(tree.credit)
#Plot the tree
plot(tree.credit)
text(tree.credit, pretty = 0)
#Calculating test-MSE
y.test = data.test$Balance
pred.credit = predict(tree.credit, data.test)
MSE.test = mean((y.test - pred.credit)^2)
MSE.test
((MSE.test)^0.5)
# Cross-validation
cv.credit = cv.tree(tree.credit, FUN = prune.tree)
par(mfrow = c(1,2))
plot(cv.credit$size, cv.credit$dev, type = "b")
plot(cv.credit$k, cv.credit$dev, type = "b")
# Re-fitting the tree with 5 nodes
prune.credit = prune.tree(tree.credit, best =5 )
plot(prune.credit)
text(prune.credit, pretty = 0)
# Use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Pruning the tree causes the test-MSE to increase to 46776.09
#Fit the model with Bagging
bag.credit = randomForest(Balance ~., data=data1,
subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
# SUPPORT VECTOR CLASSIFIER
library(e1071)
set.seed(6822651)
#Loading data
data2= read.csv("Credit.csv")
#Create a binary variable
Y = ifelse(data2$Balance > median(data2$Balance), 1, 0)
data2$Balancebin= as.factor(Y)
#Fit a support vector classifier
svm.cv = tune(svm, Balancebin ~ ., data = data2, kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(svm.cv)
# Fit the model
svm.lin = svm(Balancebin ~., data = data2, kernel = "linear", cost = 0.1)
summary(svm.lin)
#Creating confusions a matricend calculating error rates
Y.hat.lin = predict(svm.lin, data2)
conf.mat.lin = table(data2$Balancebin, Y.hat.lin)
#Confusion matrice
conf.mat.lin
### Calculating test error rates
err.lin = 1 - sum(diag(conf.mat.lin)) / sum(conf.mat.lin)
err.lin
setwd("C:/Users/ladou/OneDrive/Desktop/Project Statistical Leaning")
# Loading data
library(ISLR)
library(tree)
library(MASS)
library(randomForest)
library(tidyverse)
set.seed(6822651)
#Loading data
credit_data= read.csv("Credit.csv")
# checking the missing value
sum(is.na(credit_data)) # ' there is no missing value '
View(credit_data)
## Transform data
data1 <- credit_data[, 3:13]
View(credit_data)
### Transform variables
data1 <- data1 %>%
mutate(
Gender = factor(Gender),
Student = factor(Student),
Married = factor(Married),
Ethnicity = factor(Ethnicity),
)
#Create training and test set
train =sample(1:nrow(data1), nrow(data1)*0.5)
data.train = data1[train, ]
data.test = data1[-train, ]
#Fit a regression tree to the training set.
tree.credit =tree(Balance~., data = data.train)
summary(tree.credit)
#Plot the tree
plot(tree.credit)
text(tree.credit, pretty = 0)
#Calculating test-MSE
y.test = data.test$Balance
pred.credit = predict(tree.credit, data.test)
MSE.test = mean((y.test - pred.credit)^2)
MSE.test
((MSE.test)^0.5)
# Cross-validation
cv.credit = cv.tree(tree.credit, FUN = prune.tree)
par(mfrow = c(1,2))
plot(cv.credit$size, cv.credit$dev, type = "b")
plot(cv.credit$k, cv.credit$dev, type = "b")
# Re-fitting the tree with 5 nodes
prune.credit = prune.tree(tree.credit, best =5 )
plot(prune.credit)
text(prune.credit, pretty = 0)
# Use Cross-validation  in order to determine the optimal level of tree complexity
pred.prune = predict(prune.credit, data.test)
MSE.test.prune = mean((y.test - pred.prune)^2)
MSE.test.prune
#Fit the model with Bagging
bag.credit = randomForest(Balance ~., data=data1,
subset = train,mtry=10, importance=TRUE)
bag.credit
#The test set MSE using Bagging
pred.bag = predict(bag.credit, newdata = data.test)
MSE.test.bag = mean((pred.bag - y.test)^2)
MSE.test.bag
library(e1071)
set.seed(6822651)
#Loading data
data2= read.csv("Credit.csv")
#Create a binary variable
Y = ifelse(data2$Balance > median(data2$Balance), 1, 0)
data2$Balancebin= as.factor(Y)
#Fit a support vector classifier
svm.cv = tune(svm, Balancebin ~ ., data = data2, kernel = "linear",
ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(svm.cv)
# Fit the model
svm.lin = svm(Balancebin ~., data = data2, kernel = "linear", cost = 0.1)
summary(svm.lin)
#Creating confusions a matricend calculating error rates
Y.hat.lin = predict(svm.lin, data2)
conf.mat.lin = table(data2$Balancebin, Y.hat.lin)
#Confusion matrice
conf.mat.lin
### Calculating test error rates
err.lin = 1 - sum(diag(conf.mat.lin)) / sum(conf.mat.lin)
err.lin
